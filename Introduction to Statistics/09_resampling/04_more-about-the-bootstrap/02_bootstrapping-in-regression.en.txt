Now, let's look at bootstrapping in a more complicated situation. Let's look at the case where we have pairs of data, and we look at a regression model. Remember our regression model says that the Y observation is a linear function of the X's, plus a measurement error, e. We already saw how we can use these squares, to estimate the unknown parameters A and B. And that gives us least square estimates a hat, and b hat. Now, we would like to have some standard errors for these estimates. How would we use the bootstrap to do that? The key point here is, that we don't re-sample the whole pairs of observations, but rather we re-sample the error terms. Now, of course we don't know the error terms, but what we can do is, we can use the residuals in place of the error terms. Remember, once we fit a regression line and we have observations of the line, then the residuals are simply the vertical distances to the line. And if the regression line is a good estimator, then the residual should behave similarly, to the unknown error terms. While the error terms, e, are unknown, we do know the residuals because we fit the regression line. So we can draw a bootstrap sample from those residuals, and we get n bootstrap residuals e star 1 to e star n. In the next step, we pretend that the estimated regression line is the true line, and we add on the bootstrapped residuals. That generates a new set of observations, Y star. So, the X's stay the same, but the Y stars are now different because they are generated from the X's and the estimated regression line with the bootstrap residual added on. This gives us a bootstrap sample X1, Y1 star up to X n, Yn star. And from that sample, we can estimate the parameters a hat and b hat in the usual way by least squares. And now we follow the usual bootstrap algorithm, we repeat this whole process 1000 times, and then we estimate a standard error of a hat by the standard deviation of these 1,000 estimates, a hat star, and likewise for b hat.