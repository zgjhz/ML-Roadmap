When the sample size is small, then we need to apply a small modification to the z-test. Let's look at this example. The health guideline for lead in drinking water is a concentration of not more than 15 parts per billion. Now, suppose we have five independent samples from a reservoir and they average to 15.6 parts per billion. So that's above the threshold of 15 parts per billion, but it could be due to measurement error. So the question is, is this sufficient evidence to conclude that the concentration mu in the reservoir is above the standard of 15 parts per billion? So the first thing here is to think how our measurements come about. Remember the model we had for measurements. Measurement is the true value mu plus some chance error. And that chance error explains why it's possible to get an average of 15.6 even though the true concentration in the reservoir mu might actually be 15 or below. So, a null hypothesis would be that the true concentration mu is 15 parts per billion, and the alternative would be that it is larger than 15. So by the way, why are we looking at a null where the mu is exactly 15? The reason is that this is kind of a worst case scenario. The p-value we get in that case is larger than the p-value we would get for smaller null hypotheses. And so, if we can reject in this case, we can also reject in others. So, let's try a z-test for this case. The z-statistic looks at the observed, and in this case, we're looking at averages. So we have observed average minus expected average over the standard error of the average. So we know the observed average is 15.6. We know the expected average, that's simply the value under the null hypothesis, is 15. And the only thing we need to figure out is, what's the standard error of the average? We do have a formula for the standard error of the average. It's simply the standard deviation sigma divided by square root sample size. But now the problem is that we do not know the standard deviation sigma of the measurement error. As usual, we would simply estimate sigma by s, which is the sample standard deviation of the measurements. Here's where the complication comes in. For small sample sizes, let's say smaller than 20, if we have to estimate sigma, then the normal curve is not a good enough approximation to the distribution of the z-statistic. What happens is that the uncertainty in estimating sigma throws off the approximation. In that case, we have to use a slightly different approximation which is called student's t-distribution with n - 1 degrees of freedom. Here's a graph that shows several student curves. The black curve with mu equals infinity actually corresponds to the standard normal curve. So, what you can see is that the t-curves are somewhat lower in the middle, but the tails of the t-curves are higher up. These fatter tails simply reflect the additional uncertainty that comes in from estimating sigma. If the sample size is large enough, then this additional uncertainty is not relevant, and therefore, we can simply use the standard normal curve. By the way, there's an interesting story as to why this whole thing is called students' distribution. It was actually derived by a beer brewer with the name Gosset, and he was working for the British Guinness Brewery. But the brewery did not allow its employees to make their work public, so he published these results under the pseudonym Student. When using the t-test, keep in mind that your estimate, s, for the sample standard deviation has n - 1 in the denominator, not n. That's really important for the t-test. The way you should think about the t-test is as a special case of the z-test that becomes necessary when the sample size is small. In that case, it's also better to replace the usual normal confidence intervals by a t-interval. That is, instead of using a cut of z, we would use the appropriate cut of t with n - 1 degrees of freedom. The name degrees of freedom is simply a technical term, that means, for every sample size n, there is a student curve. And the one we are going to use is the one with n - 1 degrees of freedom.