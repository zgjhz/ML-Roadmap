WEBVTT

1
00:00:00.600 --> 00:00:04.810
Последнее видео мы обнаружили, что минимизация среднеквадратической ошибки реконструкции

2
00:00:04.810 --> 00:00:08.840
эквивалентна минимизации дисперсии данных

3
00:00:08.840 --> 00:00:12.380
при проецировании на подпространство, которое будет игнорировать в PCA.

4
00:00:12.380 --> 00:00:17.530
В этом видео мы будем использовать это понимание и определить ортонормальную основу

5
00:00:17.530 --> 00:00:22.200
N-мерного принципа подпространства, используя результаты, полученные ранее.

6
00:00:22.200 --> 00:00:27.825
Здесь мы можем написать

7
00:00:27.825 --> 00:00:32.607
нашу функцию потерь, как

8
00:00:32.607 --> 00:00:37.666
он суммирует j = M + 1 в d

9
00:00:37.666 --> 00:00:42.730
bJ транспонировать как раз

10
00:00:42.730 --> 00:00:47.511
BJ, где s является

11
00:00:47.511 --> 00:00:52.600
матрицей ковариации данных.

12
00:00:52.600 --> 00:00:57.160
Сведение к минимуму этой цели требует от нас найти ортонормальную основу, которая охватывает

13
00:00:57.160 --> 00:01:00.830
подпространство, которое мы будем игнорировать, и когда у нас есть эта основа,

14
00:01:00.830 --> 00:01:05.850
мы принимаем ее ортогональное дополнение в качестве основы основного подпространства.

15
00:01:05.850 --> 00:01:10.560
Помните, что ортогональное дополнение подпространства вы состоят из всех векторов в

16
00:01:10.560 --> 00:01:15.850
исходном векторном пространстве там, ортогонально каждому векторе в вас.

17
00:01:15.850 --> 00:01:20.250
Начнем с примера, чтобы определить бивекторы и

18
00:01:20.250 --> 00:01:25.700
давайте начнем в двух измерениях, где мы хотим найти одномерное подпространство,

19
00:01:25.700 --> 00:01:31.040
такое при дисперсии данных при проецировании на это подпространство сводится к минимуму.

20
00:01:31.040 --> 00:01:36.347
Итак, мы смотрим на два базисных вектора b1 и b2 в наших двух, поэтому

21
00:01:36.347 --> 00:01:43.500
b1 и b2, а b1 будет тратить принципиальное подпространство и

22
00:01:43.500 --> 00:01:47.800
b2 его ортогональное дополнение, что означает подпространство, которое мы будем игнорировать.

23
00:01:47.800 --> 00:01:52.825
У нас также есть ограничение, что b1 и b2 являются ортонормальными, что означает, что

24
00:01:52.825 --> 00:01:57.380
bi transposed раз bj является delta ij,

25
00:01:57.380 --> 00:02:04.900
что означает, что этот точечный продукт равен 1, если i равно J и 0 в противном случае.

26
00:02:04.900 --> 00:02:11.510
В нашем примере, без двух векторов, наша функция потери J является

27
00:02:11.510 --> 00:02:16.845
b2 транспонирование раз S раз

28
00:02:16.845 --> 00:02:21.892
b2 с ограничением, что b2 транспонирование

29
00:02:21.892 --> 00:02:25.750
раз b2 равно 1.

30
00:02:25.750 --> 00:02:30.160
Для решения этой задачи оптимизации мы записываем Лагранж, а

31
00:02:30.160 --> 00:02:36.415
Лагранж - b2, транспонируем S b2 плюс лямбда раз 1- b2 транспонирование

32
00:02:36.415 --> 00:02:42.750
раз b2, где лямбда - множитель легкого диапазона.

33
00:02:42.750 --> 00:02:47.800
Итак, теперь мы вычисляем градиенты Лагранжа по отношению к b2 и по отношению к

34
00:02:47.800 --> 00:02:53.560
лямбда и устанавливаем их в 0, поэтому dL d лямбда

35
00:02:53.560 --> 00:02:59.300
составляет 1- b2 транспонирование раз b2,

36
00:02:59.300 --> 00:03:03.962
и это равно нулю, если и только если b2 транспонирование

37
00:03:03.962 --> 00:03:08.900
раз b2 равно 1, поэтому мы восстанавливаем наше ограничение.

38
00:03:08.900 --> 00:03:15.080
Итак, теперь давайте посмотрим на частичную производную L относительно 2,

39
00:03:15.080 --> 00:03:21.580
поэтому мы получаем 2b2 раз транспонирование S из первого срока и

40
00:03:21.580 --> 00:03:28.780
минус два лямбда b2 транспонирование из второго срока.

41
00:03:28.780 --> 00:03:33.320
И это должно быть нулем, и это равно нулю, если и

42
00:03:33.320 --> 00:03:40.215
только если S раз b2 равно лямбда раз b2.

43
00:03:41.420 --> 00:03:44.590
Здесь мы в конечном итоге с проблемой собственного значения, b2

44
00:03:44.590 --> 00:03:49.110
является собственным вектором матрицы ковариации данных и случай множителя ранчо собаки.

45
00:03:49.110 --> 00:03:53.910
Роль соответствующего собственного значения, если мы теперь

46
00:03:53.910 --> 00:03:58.960
вернемся к нашей функции потери, мы можем использовать это выражение.

47
00:03:58.960 --> 00:04:03.452
Вы можете написать J, который был b2

48
00:04:03.452 --> 00:04:08.260
транспонировать раз S раз b2,

49
00:04:08.260 --> 00:04:14.300
теперь мы знаем, что как раз b2 может быть записан как лямбда раз b2.

50
00:04:14.300 --> 00:04:22.825
Таким образом, мы получаем b2 транспонирование раз b2 раз лямбда и

51
00:04:22.825 --> 00:04:29.080
поскольку у нас есть ортонормальная основа,

52
00:04:29.080 --> 00:04:33.390
мы в конечном итоге с лямбдой в качестве нашей функции потери.

53
00:04:33.390 --> 00:04:37.660
Таким образом, среднеквадратичная ошибка восстановления сводится к минимуму, если

54
00:04:37.660 --> 00:04:42.780
Lambda является наименьшим собственным значением матрицы ковариации данных.

55
00:04:42.780 --> 00:04:47.720
И это означает, что нам нужно выбрать b2 в качестве соответствующего собственного вектора, и

56
00:04:47.720 --> 00:04:52.600
что мы потратим подпространство, которое мы будем игнорировать.

57
00:04:52.600 --> 00:04:56.570
Быть тем, который охватывает принципиальное подпространство, то собственный вектор, который принадлежит

58
00:04:56.570 --> 00:05:00.170
наибольшему собственному значению матрицы ковариации данных.

59
00:05:00.170 --> 00:05:04.580
Имейте в виду, что собственные векторы матрицы ковариации уже ортогональны

60
00:05:04.580 --> 00:05:08.210
друг другу из-за симметрии матрицы ковариации.

61
00:05:08.210 --> 00:05:12.450
Итак, если мы посмотрим на двухмерный пример, если это наши данные,

62
00:05:12.450 --> 00:05:17.600
то лучшая проекция, которую мы можем получить, которая сохраняет большую часть информации,

63
00:05:17.600 --> 00:05:24.220
это та, которая проецируется на подпространство, которое тратит собственный вектор.

64
00:05:24.220 --> 00:05:28.850
Из матрицы ковариации данных, которая принадлежит к наибольшему собственному значению и что

65
00:05:28.850 --> 00:05:33.880
указывается этой длинной стрелкой здесь. Теперь перейдем к общему случаю, если мы

66
00:05:33.880 --> 00:05:38.960
хотим найти принцип M мерного подпространства D мерного набора данных и

67
00:05:38.960 --> 00:05:43.588
решим для базисных векторов bj.

68
00:05:43.588 --> 00:05:48.250
Где jequals M+1 два

69
00:05:48.250 --> 00:05:52.730
D мы оптимизируем эти, мы

70
00:05:52.730 --> 00:05:57.670
в конечном итоге с теми же проблемами собственного значения, что у нас было ранее с простым примером.

71
00:05:57.670 --> 00:06:02.860
Мы в конечном итоге с S раз bj равно лямбда

72
00:06:02.860 --> 00:06:07.600
j раз bj, для

73
00:06:07.600 --> 00:06:11.767
j = M + 1.

74
00:06:11.767 --> 00:06:17.880
2D, а функция потери задается суммой соответствующих собственных значений.

75
00:06:17.880 --> 00:06:23.160
Таким образом, мы можем ездить j это сумма от M + 1

76
00:06:23.160 --> 00:06:28.300
до D всех лямбда j.

77
00:06:28.300 --> 00:06:32.500
Кроме того, в общем случае, средняя ошибка реконструкции минимизируется, если мы

78
00:06:32.500 --> 00:06:36.960
выбираем базисные векторы, которые охватывают игнорируемое подпространство, чтобы быть собственными векторами

79
00:06:36.960 --> 00:06:41.470
матрицы ковариации данных, которые принадлежат наименьшему собственные значения.

80
00:06:41.470 --> 00:06:46.250
Это эквивалентно означает, что принцип подпространства тратится собственными векторами,

81
00:06:46.250 --> 00:06:51.090
принадлежащими к M наибольшим собственным значениям матрицы ковариации данных,

82
00:06:51.090 --> 00:06:54.670
это красиво выравнивается со свойствами матрицы ковариации.

83
00:06:54.670 --> 00:06:58.600
Собственные векторы матрицы ковариаций ортогональны друг другу по причине

84
00:06:58.600 --> 00:07:03.010
симметрии. А собственный вектор, принадлежащий наибольшему

85
00:07:03.010 --> 00:07:06.600
точкам собственного значения в направлении данных с наибольшей дисперсией и

86
00:07:06.600 --> 00:07:10.770
дисперсией в этом направлении задается соответствующим собственным значением.

87
00:07:10.770 --> 00:07:14.930
Аналогичным образом, собственный вектор принадлежит второй по величине точки собственного значения

88
00:07:14.930 --> 00:07:19.220
в направлении второй по величине дисперсии данных и так далее.

89
00:07:19.220 --> 00:07:23.690
В данном видео мы определили ортонормальную основу основного

90
00:07:23.690 --> 00:07:28.290
подпространства как собственные векторы матрицы ковариации данных, которые связаны с

91
00:07:28.290 --> 00:07:33.520
самыми большими собственными значениями. В следующем видео мы соберем все части вместе и

92
00:07:33.520 --> 00:07:36.090
проработаем алгоритм PCA в деталях.