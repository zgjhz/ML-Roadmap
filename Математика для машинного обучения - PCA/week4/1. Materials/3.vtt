WEBVTT

1
00:00:01.810 --> 00:00:07.225
В этом видео мы познакомим вас с настройкой PCA и идеей высокого уровня.

2
00:00:07.225 --> 00:00:11.920
Предположим, что у нас есть набор данных X в RD, состоящий из n векторов.

3
00:00:11.920 --> 00:00:17.420
Итак, X — это набор данных, и у нас есть n векторов от X_1 до

4
00:00:17.420 --> 00:00:23.985
X_n, где X_i — D-мерные векторы.

5
00:00:23.985 --> 00:00:27.590
Наша цель — найти низкоразмерное представление

6
00:00:27.590 --> 00:00:31.200
данных, максимально похожее на X.

7
00:00:31.200 --> 00:00:35.495
Прежде чем мы начнем, давайте кратко рассмотрим три важных понятия.

8
00:00:35.495 --> 00:00:38.190
Во-первых, каждый вектор в RD можно

9
00:00:38.190 --> 00:00:42.065
представить как линейную комбинацию базисных векторов.

10
00:00:42.065 --> 00:00:44.420
Итак, давайте запишем это.

11
00:00:44.420 --> 00:00:49.555
Итак, мы пишем, что x_n можно записать как сумму

12
00:00:49.555 --> 00:00:55.975
i, равную единице D бета-версии i_n, умноженной на b_i.

13
00:00:55.975 --> 00:01:02.990
Далее мы будем считать, что b_i являются ортонормальными основаниями RD.

14
00:01:02.990 --> 00:01:05.580
Если мы предположим, что в

15
00:01:05.580 --> 00:01:09.225
произведении используются точечные программы, а b_1 — b_d являются ортонормальными основами,

16
00:01:09.225 --> 00:01:15.970
мы также можем записать бета-версию i_n в виде X_n, умноженной на b_i,

17
00:01:15.970 --> 00:01:19.880
что означает, что бета-i_n можно интерпретировать как

18
00:01:19.880 --> 00:01:22.600
ортогональную проекцию x_n на

19
00:01:22.600 --> 00:01:26.420
одномерное подпространство, охватываемое i-тым базисным вектором.

20
00:01:26.420 --> 00:01:30.000
Третье свойство заключается в том, что если у нас есть ортонормальный базис от b_1

21
00:01:30.000 --> 00:01:34.090
до b_m от RD и мы определяем

22
00:01:34.090 --> 00:01:44.265
b как матрицу, состоящую из этих ортонормальных базисных векторов.

23
00:01:48.770 --> 00:01:56.410
Затем проекцию X на подпространство

24
00:01:56.410 --> 00:02:00.270
можно записать в виде B, умноженной на B, умноженную на B раз транспонирования X. Это

25
00:02:00.270 --> 00:02:05.045
означает, что тильда X — это ортогональная проекция X на подпространство, охватываемое базисными векторами M.

26
00:02:09.660 --> 00:02:16.445
А B раз транспонирования X — это

27
00:02:16.445 --> 00:02:18.770
координаты тильды X по отношению к базисным векторам, собранным в матрице B.

28
00:02:18.770 --> 00:02:25.025
Это также называется кодом, то есть координатами или кодом.

29
00:02:25.025 --> 00:02:28.055
Теперь давайте взглянем на PCA.

30
00:02:28.055 --> 00:02:30.900
Основная идея PCA состоит в том, чтобы найти

31
00:02:30.900 --> 00:02:33.840
низкоразмерное представление X_n тильды X_n,

32
00:02:33.840 --> 00:02:37.355
которое можно выразить, используя меньшее количество базисных векторов,

33
00:02:37.355 --> 00:02:40.490
скажем, M. Мы предполагаем,

34
00:02:40.490 --> 00:02:42.630
что данные центрированы, то есть среднее значение набора данных равно нулю.

35
00:02:42.630 --> 00:02:48.050
Кроме того, мы предполагаем, что от b_1 до b_d являются ортонормальными основаниями RD.

36
00:02:48.050 --> 00:02:54.590
Как правило, мы можем записать любую тильду X_n следующим образом.

37
00:03:04.990 --> 00:03:11.385
Тильду x_n можно записать в виде суммы i, равной единице M

38
00:03:11.385 --> 00:03:21.615
бета-версии i_n, умноженной на b_i, плюс

39
00:03:21.615 --> 00:03:27.825
сумма i равна M плюс единице к D бета-версии i_n, умноженной на b_i. Все это до сих пор живёт в RD.

40
00:03:27.825 --> 00:03:33.945
Итак, мы в общем случае записали в RD любой вектор, принадлежащий первому свойству,

41
00:03:33.945 --> 00:03:38.510
и разделили сумму в свойстве 1 на две суммы.

42
00:03:38.510 --> 00:03:44.315
Один из них живет в M-мерном подпространстве, а другой — в

43
00:03:44.315 --> 00:03:47.040
D-минус M-мерном подпространстве, которое

44
00:03:47.040 --> 00:03:50.415
является ортогональным дополнением к этому конкретному подпространству.

45
00:03:53.835 --> 00:03:57.005
В PCA мы игнорируем второй термин, поэтому избавляемся от этой части.

46
00:03:57.005 --> 00:04:00.570
Затем подпространство, охватываемое

47
00:04:00.570 --> 00:04:06.705
базисными векторами от b_1 до B_m, мы называем главным подпространством.

48
00:04:10.600 --> 00:04:22.610
Таким образом, значения b_1 и b_m охватывают основное подпространство.

49
00:04:27.185 --> 00:04:32.095
Хотя тильда X_n все еще является D-мерным вектором,

50
00:04:32.095 --> 00:04:36.230
она находится в M-мерном подпространстве RD и имеет только M координат; для её представления необходимы бета n_1 или бета N_m.

51
00:04:36.230 --> 00:04:40.505
Итак, эти значения являются координатами этого вектора тильды X_n.

52
00:04:44.540 --> 00:04:47.960
Бета-значения n также называют кодом координат

53
00:04:47.960 --> 00:04:51.375
тильды x_n по отношению к базисным векторам b_1 — b_M, и теперь эта настройка выглядит следующим образом.

54
00:04:55.365 --> 00:05:02.480
Предполагая, что у нас есть данные от X_1 до x_n,

55
00:05:02.480 --> 00:05:06.540
мы хотим найти параметры beta i_n и ортонормальные базисные векторы b_i, например, при среднем квадрате эпохи реконструкции сведены к минимуму.

56
00:05:06.540 --> 00:05:10.720
А среднеквадратичную ошибку реконструкции можно записать следующим образом.

57
00:05:10.720 --> 00:05:14.620
Мы можем записать J,

58
00:05:14.620 --> 00:05:17.195
то

59
00:05:17.195 --> 00:05:23.455
есть среднеквадратичную ошибку реконструкции, равную единице в N, умноженной на сумму n, равную единице N,

60
00:05:23.455 --> 00:05:30.730
а затем записать X_n минус X_n в квадрате тильды.

61
00:05:30.730 --> 00:05:33.280
Давайте рассмотрим пример.

62
00:05:33.280 --> 00:05:36.480
У нас есть данные, живущие в двух измерениях, и теперь мы хотим

63
00:05:36.480 --> 00:05:40.500
найти хорошее одномерное подпространство, чтобы свести к

64
00:05:40.500 --> 00:05:44.020
минимуму квадратичную или среднеквадратичную ошибку восстановления

65
00:05:44.020 --> 00:05:48.060
исходных точек данных и их соответствующей проекции.

66
00:05:48.060 --> 00:05:51.350
Здесь я строю исходный набор данных с

67
00:05:51.350 --> 00:05:53.340
соответствующими проекциями на

68
00:05:53.340 --> 00:05:57.220
одномерные подпространства и просматриваю несколько вариантов

69
00:05:57.220 --> 00:06:02.360
подпространств, и вы увидите, что некоторые из этих проекций

70
00:06:02.360 --> 00:06:08.570
значительно информативнее других, и в PCA мы найдем лучшую из них.

71
00:06:08.570 --> 00:06:15.615
Наш подход заключается в вычислении частных производных J по параметрам.

72
00:06:15.615 --> 00:06:22.310
В качестве параметров используются бета-версии i_n и b_i.

73
00:06:22.310 --> 00:06:25.360
Мы

74
00:06:25.360 --> 00:06:29.735
обнуляем частные производные J по этим параметрам и находим оптимальные параметры.

75
00:06:29.735 --> 00:06:32.555
Но одно наблюдение мы уже можем сделать.

76
00:06:32.555 --> 00:06:37.495
Замечание состоит в том, что параметры

77
00:06:37.495 --> 00:06:42.465
входят в эту функцию потерь только через тильду X_n.

78
00:06:42.465 --> 00:06:45.225
Это означает, что для получения наших частных производных

79
00:06:45.225 --> 00:06:47.490
необходимо применить правило цепочки.

80
00:06:57.355 --> 00:07:04.520
Таким образом, мы можем записать d_j через d, либо beta i_n, либо

81
00:07:04.520 --> 00:07:14.800
b_i можно записать как d_j по

82
00:07:14.800 --> 00:07:22.030
тильде d_x_n, тильда D_x_n тильда по d либо бета i_n, либо b_i.

83
00:07:23.370 --> 00:07:30.625
А первая часть, которую мы уже можем вычислить, и мы получим

84
00:07:30.625 --> 00:07:40.220
d_j за d_x_n тильда, равна минус двум более N умноженным на X_n минус транспонирование тильды x_n.

85
00:07:40.220 --> 00:07:44.940
А другие деривативы мы вычисляем в следующих видео.