WEBVTT

1
00:00:01.750 --> 00:00:04.760
Мы выводим PCA с точки зрения

2
00:00:04.760 --> 00:00:07.151
минимизации среднеквадратичной ошибки реконструкции.

3
00:00:07.151 --> 00:00:11.960
Однако PCA также можно интерпретировать с разных точек зрения.

4
00:00:11.960 --> 00:00:15.860
В этом видео мы кратко рассмотрим некоторые из этих интерпретаций.

5
00:00:15.860 --> 00:00:17.555
Давайте начнем с краткого обзора

6
00:00:17.555 --> 00:00:19.415
того, что мы уже сделали.

7
00:00:19.415 --> 00:00:26.370
Мы взяли вектор высокой размерности x и спроецировали его на

8
00:00:26.370 --> 00:00:33.655
более низкое представление z с помощью транспонирования матрицы B.

9
00:00:33.655 --> 00:00:37.010
Столбцы этой матрицы B представляют собой собственные векторы

10
00:00:37.010 --> 00:00:42.105
ковариационной матрицы данных, связанные с наибольшими собственными значениями.

11
00:00:42.105 --> 00:00:46.175
Значения z — это координаты

12
00:00:46.175 --> 00:00:51.120
нашей точки данных по отношению к базисным векторам, охватывающим главное подпространство.

13
00:00:51.120 --> 00:00:55.860
Это также называется кодом нашей точки данных.

14
00:00:55.860 --> 00:00:58.002
Получив представление z в более низкой размерности,

15
00:00:58.002 --> 00:01:05.110
мы можем получить его версию с более высокой размерностью, снова используя матрицу B.

16
00:01:05.110 --> 00:01:09.590
Таким образом, умножив B на z, мы получим

17
00:01:09.590 --> 00:01:17.090
версию z в исходном пространстве данных с более высокой размерностью.

18
00:01:17.090 --> 00:01:23.140
Мы обнаружили такие параметры PCA, что ошибка восстановления между x и

19
00:01:23.140 --> 00:01:30.840
тильдой восстановления x сведена к минимуму.

20
00:01:30.840 --> 00:01:35.275
Мы также можем рассматривать PCA как линейный автоэнкодер.

21
00:01:35.275 --> 00:01:39.900
Автоэнкодер кодирует точку данных x и

22
00:01:39.900 --> 00:01:44.560
пытается декодировать ее во что-то похожее на ту же точку данных.

23
00:01:44.560 --> 00:01:51.405
Сопоставление данных с кодом называется кодировщиком. Давайте запишем это.

24
00:01:51.405 --> 00:01:57.830
Итак, эта часть здесь называется кодировщиком,

25
00:01:58.610 --> 00:02:06.710
а отображение кода в исходное пространство данных называется декодером.

26
00:02:07.630 --> 00:02:11.790
Если кодировщик и декодер представляют собой линейные отображения,

27
00:02:11.790 --> 00:02:17.390
то решение PCA будет получено, если свести к минимуму квадратичные потери при автокодировании.

28
00:02:21.415 --> 00:02:23.860
Если мы заменим линейное отображение PCA нелинейным отображением, мы получим нелинейный автоэнкодер.

29
00:02:23.860 --> 00:02:27.850
Ярким примером этого является глубокий автоэнкодер, в котором

30
00:02:27.850 --> 00:02:34.480
линейные функции кодера и декодера заменены глубокими нейронными сетями.

31
00:02:34.480 --> 00:02:39.810
Другая интерпретация PCA связана с теорией информации.

32
00:02:39.810 --> 00:02:45.490
Код можно рассматривать как уменьшенную сжатую версию исходной точки данных.

33
00:02:45.490 --> 00:02:48.930
Восстанавливая исходные данные с помощью кода,

34
00:02:48.930 --> 00:02:51.060
мы получаем не точную точку данных,

35
00:02:51.060 --> 00:02:54.555
а слегка искаженную или зашумленную версию.

36
00:02:54.555 --> 00:02:56.604
Это означает, что наше сжатие происходит с потерями.

37
00:02:56.604 --> 00:02:59.850
Интуитивно мы хотим максимизировать

38
00:02:59.850 --> 00:03:04.340
корреляцию между исходными данными и кодом более низкой размерности. В

39
00:03:04.340 --> 00:03:08.545
более формальном плане это будет связано с взаимной информацией.

40
00:03:08.545 --> 00:03:12.340
Затем мы найдем то же решение проблемы PCA, которое мы обсуждали

41
00:03:12.340 --> 00:03:15.900
ранее в этом курсе, путем максимизации взаимной информации, что

42
00:03:15.900 --> 00:03:18.670
является основной концепцией теории информации.

43
00:03:18.670 --> 00:03:21.415
При расчете коэффициента PCA с помощью проекций

44
00:03:21.415 --> 00:03:25.840
мы переформулировали среднюю величину потерь от ошибок восстановления, чтобы свести к минимуму

45
00:03:25.840 --> 00:03:28.360
дисперсию данных, проецируемых на

46
00:03:28.360 --> 00:03:31.570
ортогональное дополнение основного подпространства.

47
00:03:31.570 --> 00:03:35.380
Минимизация этой дисперсии равносильна максимизации дисперсии данных при проецировании

48
00:03:35.380 --> 00:03:39.805
на основное подпространство.

49
00:03:39.805 --> 00:03:44.380
Если рассматривать разницу в данных как информацию, содержащуюся в данных,

50
00:03:44.380 --> 00:03:46.990
это означает, что PCA также можно

51
00:03:46.990 --> 00:03:51.955
интерпретировать как метод, позволяющий сохранить как можно больше информации.

52
00:03:51.955 --> 00:03:56.280
Мы также можем взглянуть на PCA с точки зрения модели латентных переменных.

53
00:03:56.280 --> 00:04:02.800
Мы предполагаем, что неизвестный код низкой размерности z генерирует данные

54
00:04:02.800 --> 00:04:09.805
x, и предположим, что между z и x существует линейная зависимость

55
00:04:09.805 --> 00:04:19.020
, поэтому в общем случае можно записать, что x равно B умноженному на z плюс mu и, возможно, некоторый шум.

56
00:04:19.020 --> 00:04:21.915
Мы предполагаем, что шум

57
00:04:21.915 --> 00:04:26.880
изотропный со средним нулем и квадратом ковариационной матрицы, умноженным на сигму I.

58
00:04:26.880 --> 00:04:33.308
Далее мы предполагаем, что распределение z является стандартной нормалью, поэтому P от z равно

59
00:04:33.308 --> 00:04:41.210
гауссову, а среднее значение равно нулю, а ковариационная матрица — матрица идентичности.

60
00:04:41.210 --> 00:04:44.423
Теперь мы можем записать вероятность этой модели.

61
00:04:44.423 --> 00:04:53.555
Итак, вероятность равна P от x при заданном z, а это распределение Гаусса в x

62
00:04:53.555 --> 00:04:57.150
со средним значением Bz плюс

63
00:04:57.150 --> 00:05:04.685
mu и квадратом ковариационной матрицы sigma I.

64
00:05:04.685 --> 00:05:10.470
Кроме того, мы можем вычислить предельное правдоподобие, поскольку P

65
00:05:10.470 --> 00:05:16.122
от x является интегралом P от x при заданном z.

66
00:05:16.122 --> 00:05:20.190
Таким образом, это вероятность, умноженная на распределение по z, dz,

67
00:05:20.190 --> 00:05:28.110
и это распределение Гаусса в x со

68
00:05:28.110 --> 00:05:37.114
средним значением mu и с ковариационной матрицей, умноженной на B B транспонирует плюс квадрат сигмы I.

69
00:05:37.114 --> 00:05:45.250
Параметры этой модели

70
00:05:45.250 --> 00:05:49.630
представлены в квадратах mu, B и сигма.

71
00:05:49.630 --> 00:05:57.130
И мы можем прямо записать их в нашей модели здесь.

72
00:05:57.130 --> 00:06:01.650
Таким образом, параметры модели расположены в квадратах B, mu и сигма.

73
00:06:01.650 --> 00:06:13.910
Теперь мы можем определить параметры этой модели,

74
00:06:13.910 --> 00:06:16.595
используя оценку максимального правдоподобия,

75
00:06:16.595 --> 00:06:21.230
и обнаружим, что mu — среднее значение данных, а B —

76
00:06:21.230 --> 00:06:26.055
матрица, содержащая собственные векторы, соответствующие наибольшим собственным значениям.

77
00:06:26.055 --> 00:06:28.445
Чтобы получить код низкой размерности точки данных,

78
00:06:28.445 --> 00:06:33.815
мы можем применить теорему Байеса для инвертирования линейной зависимости между z и x.

79
00:06:33.815 --> 00:06:38.977
В частности, мы получим P из z,

80
00:06:38.977 --> 00:06:44.830
если x равно P из x, заданное z.

81
00:06:44.830 --> 00:06:51.478
Таким образом, эта вероятность исходит отсюда, умноженная на P от z.

82
00:06:51.478 --> 00:06:55.790
Итак, вот наше распределение, которое мы имеем здесь.

83
00:06:55.790 --> 00:07:04.441
Делится на полученное отсюда предельное правдоподобие P числа x.

84
00:07:08.550 --> 00:07:14.640
В этом видео мы рассмотрели пять различных аспектов

85
00:07:14.640 --> 00:07:17.205
PCA, которые преследуют разные цели: минимизация квадратичной ошибки восстановления, минимизация

86
00:07:17.205 --> 00:07:19.890
потерь в автоэнкодере, максимизация

87
00:07:19.890 --> 00:07:22.643
взаимной информации, максимизация дисперсии

88
00:07:22.643 --> 00:07:26.580
проецируемых данных и максимизация вероятности в модели скрытых переменных.

89
00:07:26.580 --> 00:07:31.620
Все эти разные точки зрения дают нам одно и то же решение проблемы PCA.

90
00:07:35.040 --> 00:07:41.250
Когда мы рассматриваем свойства реальных данных, сильные и слабые стороны отдельных точек зрения становятся более очевидными и важными.