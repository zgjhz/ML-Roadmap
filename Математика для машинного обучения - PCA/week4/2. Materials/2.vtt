WEBVTT

1
00:00:01.630 --> 00:00:06.390
В последнем видео мы рассмотрели шаги алгоритма PCA.

2
00:00:06.390 --> 00:00:08.225
Чтобы выполнить PCA,

3
00:00:08.225 --> 00:00:10.880
нам нужно вычислить матрицу ковариации данных.

4
00:00:10.880 --> 00:00:14.960
В измерениях D ковариационная матрица данных представляет собой матрицу D на D.

5
00:00:14.960 --> 00:00:16.660
Если D очень велико, то есть имеет

6
00:00:16.660 --> 00:00:18.095
очень большие размерности,

7
00:00:18.095 --> 00:00:22.765
то вычисление собственных значений и собственных векторов этой матрицы может быть довольно дорогостоящим.

8
00:00:22.765 --> 00:00:25.888
Он сканирует кубически по количеству строк и столбцов,

9
00:00:25.888 --> 00:00:28.525
в данном случае по количеству измерений.

10
00:00:28.525 --> 00:00:32.090
В этом видео мы предлагаем решение этой проблемы в

11
00:00:32.090 --> 00:00:35.825
том случае, если точек данных значительно меньше, чем размеров.

12
00:00:35.825 --> 00:00:39.460
Предположим, что у нас есть набор данных от

13
00:00:39.460 --> 00:00:47.770
X1 до XN в RD,

14
00:00:47.770 --> 00:00:51.660
и мы предполагаем, что данные центрированы таким образом, что их среднее значение равно нулю.

15
00:00:51.660 --> 00:00:56.470
Затем матрица ковариации данных задается в виде S, равная

16
00:00:56.470 --> 00:01:02.480
1, а не N, умноженному на X,

17
00:01:02.480 --> 00:01:11.420
где X — матрица, состоящая из транспонирования X1

18
00:01:11.420 --> 00:01:21.040
до XN и представляющая собой матрицу размером N на D.

19
00:01:21.040 --> 00:01:24.630
Теперь мы предполагаем, что N значительно меньше D. Это означает, что

20
00:01:24.630 --> 00:01:29.605
количество точек данных значительно меньше размерности данных.

21
00:01:29.605 --> 00:01:37.750
Тогда ранг ковариационной матрицы равен N. Таким образом, ранг

22
00:01:37.750 --> 00:01:41.080
S равен N. Это также

23
00:01:41.080 --> 00:01:46.230
означает, что она имеет D минус N плюс 1, многие собственные значения равны нулю.

24
00:01:46.230 --> 00:01:48.475
Это означает, что матрица не является полноранговой,

25
00:01:48.475 --> 00:01:51.630
а строки и столбцы линейно зависимы.

26
00:01:51.630 --> 00:01:54.520
Другими словами, есть некоторые избыточности.

27
00:01:54.520 --> 00:01:58.330
В следующие несколько минут мы воспользуемся этим и превратим ковариационную матрицу D на

28
00:01:58.330 --> 00:02:05.875
D S в полноранговую ковариационную матрицу N на N без нулевого собственного значения.

29
00:02:05.875 --> 00:02:09.940
Я только что перенес сюда определение ковариации, чтобы на доске было немного больше места.

30
00:02:09.940 --> 00:02:15.660
В PCA мы получили следующее уравнение собственного вектора с собственными значениями.

31
00:02:15.660 --> 00:02:21.210
У нас S умноженное на bi равно лямбде I умноженное на bi,

32
00:02:21.210 --> 00:02:26.410
где bi — базисный вектор ортогонального дополнения основного подпространства.

33
00:02:26.410 --> 00:02:28.685
Теперь давайте немного перепишем это уравнение.

34
00:02:28.685 --> 00:02:31.920
Теперь мы заменим S определением, приведенным здесь.

35
00:02:31.920 --> 00:02:38.935
Таким образом, мы получим от 1 до N умноженное на X транспонирование X.

36
00:02:38.935 --> 00:02:47.155
Это значение равно S умноженному на bi равно лямбде I умноженному на bi.

37
00:02:47.155 --> 00:02:51.820
А теперь умножим x с левой стороны.

38
00:02:51.820 --> 00:03:01.955
Таким образом, мы получим умноженное на X,

39
00:03:01.955 --> 00:03:14.625
переместив Xbi, умноженное на N, равно лямбде I, умноженному на X,

40
00:03:14.625 --> 00:03:22.425
умноженное на bi.

41
00:03:22.425 --> 00:03:25.840
Теперь у нас есть новое уравнение собственных значений вектора.

42
00:03:25.840 --> 00:03:29.250
Итак, лямбда I по-прежнему остается собственным значением.

43
00:03:29.250 --> 00:03:34.965
Теперь у нас есть собственные векторы X, умноженные на bi,

44
00:03:34.965 --> 00:03:45.745
которые мы называем ci матрицы 1 более N раз, умноженным на X, и транспонируем их.

45
00:03:49.630 --> 00:03:51.870
Это означает, что транспонирование, умноженное на N, имеет

46
00:03:51.870 --> 00:03:56.650
те же ненулевые собственные значения, что и

47
00:03:56.650 --> 00:03:59.785
ковариационная матрица данных, но теперь это матрица размером N на N,

48
00:03:59.785 --> 00:04:04.100
поэтому вычислять собственные значения и собственные векторы можно намного быстрее, чем для исходной ковариационной матрицы данных.

49
00:04:11.335 --> 00:04:16.990
Итак, это матрица размером N на N, тогда как S раньше была матрицей D на D.

50
00:04:16.990 --> 00:04:23.190
Итак, теперь мы можем вычислить собственные векторы этой матрицы 1 из N, умноженных на транспонирование X X.

51
00:04:23.190 --> 00:04:29.450
И мы используем это для восстановления исходных собственных векторов, что нам все еще нужно сделать для PCA.

52
00:04:29.450 --> 00:04:34.930
В настоящее время нам известны собственные векторы, равные 1 по N, умноженному на транспонирование X X,

53
00:04:34.930 --> 00:04:39.550
и мы хотим восстановить собственные векторы S. Если мы

54
00:04:39.550 --> 00:04:44.000
умножим наше уравнение собственного вектора на значение X транспонирования, мы получим

55
00:04:44.000 --> 00:04:53.110
следующее: если мы получим значение 1 на N умноженное на X, умноженное на X, умноженное на X, умноженное на

56
00:04:53.110 --> 00:04:58.915
X, время транспонирования

57
00:04:58.915 --> 00:05:06.020
ci равно лямбде I,

58
00:05:06.020 --> 00:05:13.470
умноженное на X, умноженное на X.

59
00:05:13.470 --> 00:05:16.390
Теперь мы снова находим нашу S-матрицу.

60
00:05:25.580 --> 00:05:29.345
Это S, и это

61
00:05:29.345 --> 00:05:35.330
также означает, что мы восстанавливаем время транспонирования X,

62
00:05:35.330 --> 00:05:40.340
ci — это собственный вектор S, принадлежащий лямбде собственных значений.

63
00:05:40.340 --> 00:05:42.590
В этом видео мы переформулировали PCA так, чтобы можно было эффективно запускать PCA

64
00:05:42.590 --> 00:05:46.560
на наборах данных, но размерность данных значительно превышает количество точек данных.